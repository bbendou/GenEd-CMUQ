{"code": "98-375", "name": "Student Taught Courses (StuCo): Introduction to AI Alignment", "base_name": "Student Taught Courses (StuCo): Introduction to AI Alignment", "units": 3.0, "min_units": 3.0, "max_units": 3.0, "short_name": "STUCO: AI ALIGNMENT", "is_topic": false, "topic": null, "prereqs": {"text": "", "req_obj": null, "raw_pre_req": ""}, "offered_in_campuses": [1], "offerings": [{"campus_id": 1, "semesters": [{"semester": 1, "year": 2020}, {"semester": 2, "year": 2021}, {"semester": 1, "year": 2021}], "sub_semesters": []}], "co_reqs": [], "anti_reqs": [], "equiv": [], "long_desc": "As we delegate more and more societal responsibilities to Artificial Intelligence, we raise pressing ethical questions about what will happen if these systems arent aligned with our values. Many people, including AI experts like Stuart Russell, believe that there is an urgent need to mitigate the risks associated with future advanced AI systems and to ensure that their contributions are beneficial to humanity and the world.   We will start by exploring arguments for and against the importance of AI alignment work, especially in relation to reducing existential risk. Then, we will learn about existing AI safety technical research, efforts to implement policy measures that reduce AI risk, and how you can personally contribute to AI safety. Basic knowledge about machine learning helps but is not required.", "student_sets": [{"id": 896, "name": "undergraduate"}], "offering_tags": [{"name": "intermittent", "type": "INFO"}], "website": null, "instructors": [], "is_repeatable": false, "is_req_repeatable": false, "repeat_limit_attempts": null, "repeat_limit_credits": null, "catalog_levels": [], "skills": [], "attributes": [], "custom_fields": {"goals": "Be familiar with the basic arguments for and against existential risks due to advanced AI. Understand some of the past and current research directions within AI alignmentsafety. Form an opinion about how much you believe these arguments and whether you want to keep exploring AI safety.", "key_topics": "- AI safety\n- AI alignment (and related arguments)\n- Goal misgeneralization\n- Mechanistic interpretability\n- Artificial general intelligence\n- Inverse reinforcement learning\"", "prerequisite_knowledge": "basic ML knowledge is useful but not strictly required", "assessment_structure": "Midterm: take-home short response. This will be an open-ended prompt that will allow students to practice reasoning about AI safety and concepts learned.\n\nFinal project: the final will be a student-defined project. Projects may be technical (e.g., \"I coded up this circuit's interpretability paper to test it myself\") or more conceptual. Grading criteria will include overall quality of the project, creativity in project, and the overall progress made on the project (i.e. a finished project that has progress little over the last 3 weeks will may receive the same grade as a unfinished project that has progressed greatly).\n\nAttendance and participation: students are expected to attend every lecture (with the possibility to excuse absences) and participate in class discussions. Attendance will taken with a feedback form at the end of class.", "relevance": "AI Safety Fundamentals allows students to explore the emerging field of AI Alignment, the research field directed at ensuring advanced AI systems robustly pursue human goals. We expect that many students interested in AI still be interested in these topics, which are not commonly taught by academics due to the fact that AI Alignment is still nascent and pre-paradigmatic.", "learning_resources": "The following web pages will serve as reference material:\nhttps://www.agisafetyfundamentals.com/ai-alignment-curriculum\nhttps://www.agisafetyfundamentals.com/alignment-201-curriculum\"", "extra_time_commitment": "We will ask students to complete ~30 min - 1 hour of reading prior to each class."}, "grade_option": {"name": "Pass/Fail", "short_name": "P", "never_graded": false, "is_pass_fail": true, "is_audit": false, "id": 4}, "admin_context": {"suggested_by_advisor": null, "counts_for": []}, "success": true}